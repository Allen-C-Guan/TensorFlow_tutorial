{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators  tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立Estimator的步骤\n",
    "1. pipline\\\n",
    "    功能：将数据以bitch的方式导入veriable中，只有把数据变成tf.data的格式，数据才可以在网络里流通\\\n",
    "    定义：input_fn（features, label, training, batch_size)。\\\n",
    "    函数: tf.data.Dataset.from_tensor_slices(feature, label)\\\n",
    "    \n",
    "    \n",
    "    \n",
    "2. feature column\\\n",
    "    功能：在该步骤中，我们主要是把真实世界的描述性feature（长宽高）变成计算机能理解的label（数字，向量，矩阵等），而变换后的features将作为estimator的输入\\\n",
    "    定义：my_feature_columns.append(tf.feature_column.numeric_column(key=key_discribe))\n",
    "        说明：key_discribe是原来数据的label，key接收到以后，会转换成数字，并记住数字和原来的label之间的关系\n",
    "    函数：tf.feature_column\n",
    "\n",
    "\n",
    "3. estimator\n",
    "    功能：建立神经网络的结构（输入，输出，优化器，层数，节点个数）\n",
    "    定义：tf.estimator.DNNClassifier（feature_columns，n_classes）\n",
    "    说明：在这里feature_columns是通过tf.feature_column产生的feature,表示数据有多少个属性。 n_classes决定了输出的种类）\n",
    "   \n",
    "\n",
    "4. train\\\n",
    "    功能：让estimator跑起来！！\\\n",
    "    函数：classifier.train(\n",
    "          input_fn=lambda:input_fn(train,train_label,training=True),#pipline给输入\n",
    "          steps = 5000)\n",
    "    说明：数据流：原始数据从input_fn成批（batch）的进入到estimator中， 在estimator中，数据的原始features被一对一的自动对照着你新给的feature list，换成了数字化的features。而后参与计算\n",
    "    \n",
    "    \n",
    "5. evaluation & predition\\\n",
    "    这里需要重新定义一个input_fn_pred，去掉label参数，但evaluation的pipline可以用原来的input_fn。\n",
    "\n",
    "\n",
    "tips:\\\n",
    "        input_fn在train，evaluation中，输入的是feature数据+label，因此estimator收到的也是这两个。\\\n",
    "        对于prediction而言，estimator收到是一个，即只有feature数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一些属性\n",
    "该属性包括 长度，宽度，品种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置下载目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/allen/Desktop/tensorflow/TensorFlow session\n",
      "/Users/allen/Desktop/tensorflow\n",
      "/Users/allen/Desktop/tensorflow/database/flower\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#path = os.path.abspath(\"flower datasate\")#如果直接在abspath后面写，（不要以/开头），则是当前文件夹绝对目录的子目录\n",
    "abs_path = os.path.abspath(\"\")#当前运行文件的文件夹，\n",
    "path = os.path.dirname(abs_path)#文件夹的文件夹——父层文件夹\n",
    "print(abs_path)\n",
    "print(path)\n",
    "\n",
    "dataset_path = path+\"/database/flower\"\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dataset_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = tf.keras.utils.get_file(\n",
    "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\",\n",
    "    cache_dir = dataset_path)\n",
    "test_path = tf.keras.utils.get_file(\n",
    "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\",\n",
    "    cache_dir = dataset_path)\n",
    "\n",
    "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "0          6.4         2.8          5.6         2.2        2\n",
       "1          5.0         2.3          3.3         1.0        1\n",
       "2          4.9         2.5          4.5         1.7        2\n",
       "3          4.9         3.1          1.5         0.1        0\n",
       "4          5.7         3.8          1.7         0.3        0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1\n",
       "4          5.7         3.8          1.7         0.3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = train.pop('Species')#把species这一栏数据全剔除掉，并将剔除掉的放在test_label中\n",
    "test_label = test.pop('Species')\n",
    "\n",
    "# 标签列现已从数据中删除\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个表格是pandas，当然也可以看成dict类型的数据。  \n",
    "其中sepalLength是key，而对应该列的数都是value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据的方法\n",
    "利用from_tensor_slices 的方法， 把数据获取，并拆分第一维度，放在变量中。  \n",
    "利用dataset把数据变成dataset type的variable，可以让数据在python中流通，不然数据不加载为dataset class，那么数据无法被处理和加工。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用input_fn函数将数据分批的导入到train里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "\n",
    "    # 将输入转换为数据集。\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # 如果在训练模式下混淆并重复数据。\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "                                    #如果training状态下，需要对dataset进行shuffle\n",
    "                                    #如果不是的话，不需要shuffle。\n",
    "                                    #repeat等于把数据*count次，数据库会通过重复的方式被扩大，没有参数的时候是一直重复\n",
    "    \n",
    "    return dataset.batch(batch_size)#返回的是1个batch的数据，batch的数目决定了slices的个数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的主要功能在于：  \n",
    "1. 数据变成dataset\n",
    "2. 数据被shuffle和repeat\n",
    "3. 不管是不是train，返回的都是a batch of slices的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature column\n",
    "按feature的顺序建立一个新的数字list。\\\n",
    "tf.feature column()起到的作用是，把原始数据中的feature，比如 长度，宽度，高度等，变成适合model去理解的数据形态，比如我们可以让“1”表示长度，“2”表示宽度，“3”表示高度等，此时我们的数据形式就从：  \n",
    "高度：4  ==>  3:4\\\n",
    "这里可以看懂 3:4，很明显，计算机就很好理解了。\\\n",
    "当然，转换feature的方式并不只是一种，即名称变成数字，还有例如one-hot等，但其目的都是一样的，就是把文字数字化，成为计算机好立即的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_column.numeric_column()\n",
    "这个函数的目的是把feature变成数字化的特征。例如把“高度”变成1，或者 vector（0，1，3，4），甚至是矩阵。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.feature_column.numeric_column(\n",
    "    key,                     #key表示的是原始数据的feature名称（ie：高度）\n",
    "    shape=(1,),              #数字化后的tensor的shape\n",
    "    default_value=None,      #可以给其设置默认值\n",
    "    dtype=tf.dtypes.float32, #数字的类型，ie。int\n",
    "    normalizer_fn=None       #当默认值设定以后，可能需要对新生成的值标准化，这时候，等号后面是个\n",
    "                                function，例如，lambda x:x*2等，\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征列描述了如何使用输入。\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'], dtype='object')\n",
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "print(train.keys())\n",
    "print(my_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们已经把所有key都加载到了Numeric Column中了，这个时候，所有feature在这里面都是一个数字了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例化Estimator\n",
    "Tensorflow 已经预先给出了一些典型的estimator   \n",
    "1. tf.estimator.DNNClassifier 用于多类别分类的深度模型   \n",
    "2. tf.estimator.DNNLinearCombinedClassifier 用于广度与深度模型   \n",
    "3. tf.estimator.LinearClassifier 用于基于线性模型的分类器   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 21:47:47.995589 4585059776 estimator.py:1811] Using temporary folder as model directory: /var/folders/l4/g9rjy7n51jb1sm54js4hpjdc0000gn/T/tmptczfs9d3\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "                feature_columns=my_feature_columns,\n",
    "                hidden_units=[30,10],\n",
    "                            #说明层数的hidden_layer，第一层是30，第二层是10个\n",
    "                n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "在estimator中，其主要借鉴了sklearn的train功能，即只要用train一句话，既可完成模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and  evaluate tutorial "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train(   \n",
    "    input_fn,               # 输入的必须是（feature, label)类型的数据，不然就报错      \n",
    "    hooks=None,             # callbacks的作用      \n",
    "    steps=None,             # step表示train迭代的次数，如果没有设置，就永久迭代或者直到input_fn报错 （outofrange)为止     \n",
    "    max_steps=None,         # 最大迭代次数。     \n",
    "    saving_listeners=None   # callbacks   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0824 22:20:25.021616 4585059776 deprecation.py:323] From /Users/allen/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/head/base_head.py:574: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0824 22:20:25.193869 4585059776 deprecation.py:506] From /Users/allen/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0824 22:20:25.608551 4585059776 deprecation.py:323] From /Users/allen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1340: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x630ac50b8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:input_fn(train,train_label,training=True),\n",
    "    steps = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    ".evalute 是train的一个method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "        input_fn =  lambda:input_fn(test,test_y,training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，没有设置step，只要我们把training = false，而后就不shuffle，也不repeat了，当一个epech（全部的都遍历一遍）结束的时候，就结束evaluate了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction（without label）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立一个input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 必须要输入是 dict type的 { feature1 : [val1,val2,val3], feature2 : ... }\n",
    "2. input_fn_pred 与input_fn_eval 和 input_fn_train最大的不同是： 函数的输入没有label了！！！！ 因为predict本身也是没有label的，因此我们需要重新定义自己的input_fn来满足prediction的数据输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由模型生成预测\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "def input_fn_pred(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # 将输入转换为无标签数据集。\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: input_fn_pred(predict_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict和train和evaluate完全相同，都是从input_fn中拿一个batch的数据（多少片数据），而后放到网络中，得到数据。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is \"Setosa\" (60.4%), expected \"Setosa\"\n",
      "Prediction is \"Versicolor\" (43.1%), expected \"Versicolor\"\n",
      "Prediction is \"Virginica\" (59.1%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n",
    "        SPECIES[class_id], 100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
