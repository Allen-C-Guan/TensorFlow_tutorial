{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientTape\n",
    "自动微分器主要的功能是用来计算微分的值的，通常用于梯度下降法。\\\n",
    "除此之外，自动微分器可以用来追踪tensor的训练过程。\\\n",
    "其工作原理是在进程中执行微分操作，而后记录在tape里面，关闭了以后我们需要的值记录在tape里面，拿出来看就行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scalar的梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 16:22:52.472019 4734318016 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0902 16:22:52.473538 4734318016 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0902 16:22:52.475494 4734318016 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0902 16:22:52.480392 4734318016 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "W0902 16:22:52.483010 4734318016 backprop.py:968] Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D derivation\n",
      "dy1_dx: tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "dy2_dx: tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "dy3_dx: tf.Tensor(8.0, shape=(), dtype=float32)\n",
      "2D derivation\n",
      "tf.Tensor(14.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: id=184, shape=(), dtype=float32, numpy=14.0>, <tf.Tensor: id=185, shape=(), dtype=float32, numpy=12.0>]\n"
     ]
    }
   ],
   "source": [
    "#------------------一元梯度----------------------\n",
    "print(\"1D derivation\")\n",
    "x = tf.constant(value = 3.0)   #微分器里面必须是浮点数！！！\n",
    "with tf.GradientTape(persistent=True,watch_accessed_variables = True) as tape:\n",
    "    #persistent表示可以反复使用。 \n",
    "    tape.watch(x)  #watch确定记录的变量\n",
    "    \n",
    "    \n",
    "    #定义function\n",
    "    y1=x*2\n",
    "    y2=x*x+2\n",
    "    y3=x*x+2*x\n",
    "    \n",
    "    #求导\n",
    "    dy1_dx = tape.gradient(target=y1,sources=x)#自变量是x，函数是y\n",
    "    dy2_dx = tape.gradient(target=y2,sources=x)#自变量是x，函数是y\n",
    "    dy3_dx = tape.gradient(target=y3,sources=x)#自变量是x，函数是y\n",
    "    \n",
    "   \n",
    "    #输出\n",
    "    print(\"dy1_dx:\",dy1_dx)\n",
    "    print(\"dy2_dx:\",dy2_dx)\n",
    "    print(\"dy3_dx:\",dy3_dx)\n",
    "    \n",
    "    \n",
    "#------------------二元梯度-----------------------\n",
    "print(\"2D derivation\")\n",
    "x = tf.constant(value=3.0)\n",
    "y = tf.constant(value=2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([x,y])\n",
    "    \n",
    "    #构造函数\n",
    "    z=x*x*y+x*y\n",
    "    \n",
    "    #求导\n",
    "    dz_dx=tape.gradient(sources=x,target=z)\n",
    "    dz_dx_dy = tape.gradient(sources=[x,y],target=z)\n",
    "    \n",
    "    print(dz_dx)\n",
    "    print(dz_dx_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复杂函数的梯度求解\n",
    "其求梯度的对象不但可以是直接的函数，还可以是复合函数，矩阵函数，神经网络中复杂的函数之间的复合导数，就可以用这个方法求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#创建一个矩阵\n",
    "x = tf.ones((2,2))\n",
    "\n",
    "#在微分器里面建立需要求导的自变量和因变量\n",
    "#由于GradientTape是一个进程，因此开启进程需要记得关闭，微分器的所有操作都要\n",
    "#在with... as..里面进行。操作过后，python会自动关闭进程。\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)#reduce_sum是按行列，或者整个矩阵中每个元素都加起来的和\n",
    "    z = tf.multiply(y,y)\n",
    "    \n",
    "    # 这里我们建立了两个函数\n",
    "    # y=f(x)\n",
    "    # z=f(y)\n",
    "    # 因此z也是x的复合函数！dz_dy = t.gradient(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面， GradientTape中默认的call的次数是1，就是说使用过这个tape以后，这个tape就被抹去了，因此下面的内容不能call两次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看一个tape里面记录的内容。\n",
    "dz_dx = t.gradient(z,x)\n",
    "\n",
    "#看看对不对。\n",
    "for i in [0,1]:\n",
    "    for j in [0,1]:\n",
    "        assert dz_dx[i][j].numpy() == 8 \n",
    "        # 理论上导数矩阵所有元素都应该都是8，因此，我们判定，如果不是8的话，\n",
    "        # 这里assert就起到一个判定并中断的作用，如果表达式是false，\n",
    "        # 那么就强制产生error。\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中间变量也可以被tape记录下来。\\\n",
    "我们watch的x，但是当我调用z对y的导数的时候，y的也被记录下来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "dz_dy = t.gradient(z, y)\n",
    "print(dz_dy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### persistant\n",
    "每次调用，都会自动释放tape的内存，我们可以选择调用之后不释放，即使用persistant method。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x*x\n",
    "    z = y*y\n",
    "\n",
    "dz_dx = t.gradient(z,x)\n",
    "dy_dx = t.gradient(y,x)\n",
    "\n",
    "#释放内存，用完了就销毁tape。\n",
    "del t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 控制微分流\n",
    "在微分器打开的时候，也是可以通过python的控制语言打断并改变其微分流的。\\\n",
    "这可以实现更复杂的微分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    output = 1.0\n",
    "    for i in range(y):\n",
    "        if i > 1 and i < 5:\n",
    "            output = tf.multiply(output, x)\n",
    "    return output\n",
    "\n",
    "def grad(x, y):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        out = f(x, y)    # 可以看到f(x,y)的值并不是一个固定的函数。\n",
    "                         # f(x,y)具体是什么函数还取决于x或者y的值\n",
    "                         # 可即便是这样，我们仍然可以对这样不确定的函数求导。\n",
    "                         # 这一切的实现，都是由于即使在GradientTape打开的时候，仍然可以操控数据流。\n",
    "    return t.gradient(out, x)\n",
    "\n",
    "x = tf.convert_to_tensor(2.0)\n",
    "\n",
    "\n",
    "#检验一下答案，如果有问题就产生error。\n",
    "\n",
    "assert grad(x, 6).numpy() == 12.0\n",
    "assert grad(x, 5).numpy() == 12.0\n",
    "assert grad(x, 4).numpy() == 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高阶导数\n",
    "由于GradientTape接受嵌套，因此高阶导数可以利用tape的嵌套来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = x*x*x\n",
    "    \n",
    "    dy_dx = t2.gradient(y,x)\n",
    "d2y_dx2 = t.gradient(dy_dx,x)\n",
    "\n",
    "assert dy_dx.numpy() == 3.0\n",
    "assert d2y_dx2.numpy() == 6.0\n",
    "\n",
    "print(dy_dx.numpy(),d2y_dx2.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
