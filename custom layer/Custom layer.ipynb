{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(10, input_shape=(None, 5))\n",
    "#这个层数是：input是5维度，output是10维度，none指的是bitch的维度，none，表示待定，当输入进入的时候，才确定下来。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer的功能和结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(7, 10), dtype=float32, numpy=\n",
       "array([[-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396723,  1.7013133 ,  0.5694891 , -0.2372849 , -0.07828611],\n",
       "       [-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396723,  1.7013133 ,  0.5694891 , -0.2372849 , -0.07828611],\n",
       "       [-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396723,  1.7013133 ,  0.5694891 , -0.2372849 , -0.07828611],\n",
       "       [-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396723,  1.7013133 ,  0.5694891 , -0.2372849 , -0.07828611],\n",
       "       [-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396723,  1.7013133 ,  0.5694891 , -0.2372849 , -0.07828611],\n",
       "       [-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396723,  1.7013133 ,  0.5694891 , -0.2372849 , -0.07828611],\n",
       "       [-0.8046288 , -1.4619715 ,  0.12412614,  0.10511744, -0.76358604,\n",
       "        -0.31396717,  1.7013133 ,  0.5694891 , -0.23728484, -0.07828611]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use a layer,simply call it.\n",
    "layer(tf.ones([7, 5]))\n",
    "\n",
    "# 这里input的维度是7行，，每行5个，\n",
    "# tf中，每行是一个input。\n",
    "# 输出可以看到，维度为10，个数是7。\n",
    "# 因此可以得到结论， 7个维度是5的向量进入，7个维度为10的向量输出。\n",
    "# 矩阵计算：input7*5) *  kernel(5*10) = output(7*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-4.3174341e-01, -6.2426388e-02,  4.9955314e-01,  5.3269428e-01,\n",
       "         -6.0896969e-01, -1.6359898e-01,  2.3055810e-01,  1.4307690e-01,\n",
       "          3.7908554e-04, -4.9974978e-01],\n",
       "        [ 4.4969398e-01, -6.0918802e-01, -8.2799375e-02, -4.8984641e-01,\n",
       "         -1.3373017e-02, -4.4255409e-01,  4.3746203e-01,  6.1278433e-01,\n",
       "         -5.1469946e-01,  5.7094413e-01],\n",
       "        [-3.4984636e-01,  2.6965576e-01,  2.8539860e-01, -2.6246873e-01,\n",
       "          5.0265652e-01, -4.2444718e-01,  3.1020844e-01, -1.6323665e-01,\n",
       "         -5.3905720e-01, -3.3119601e-01],\n",
       "        [-3.3422369e-01, -5.8739686e-01, -4.9672389e-01,  5.2030653e-01,\n",
       "         -1.2811726e-01,  2.1139747e-01,  3.1991774e-01, -1.5930286e-01,\n",
       "          4.5416230e-01,  1.9101876e-01],\n",
       "        [-1.3850933e-01, -4.7261602e-01, -8.1302345e-02, -1.9556823e-01,\n",
       "         -5.1578259e-01,  5.0523561e-01,  4.0316707e-01,  1.3616741e-01,\n",
       "          3.6193043e-01, -9.3032122e-03]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.variables\n",
    "#看到的5*10的矩阵，记录的是var的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-4.3174341e-01, -6.2426388e-02,  4.9955314e-01,  5.3269428e-01,\n",
       "         -6.0896969e-01, -1.6359898e-01,  2.3055810e-01,  1.4307690e-01,\n",
       "          3.7908554e-04, -4.9974978e-01],\n",
       "        [ 4.4969398e-01, -6.0918802e-01, -8.2799375e-02, -4.8984641e-01,\n",
       "         -1.3373017e-02, -4.4255409e-01,  4.3746203e-01,  6.1278433e-01,\n",
       "         -5.1469946e-01,  5.7094413e-01],\n",
       "        [-3.4984636e-01,  2.6965576e-01,  2.8539860e-01, -2.6246873e-01,\n",
       "          5.0265652e-01, -4.2444718e-01,  3.1020844e-01, -1.6323665e-01,\n",
       "         -5.3905720e-01, -3.3119601e-01],\n",
       "        [-3.3422369e-01, -5.8739686e-01, -4.9672389e-01,  5.2030653e-01,\n",
       "         -1.2811726e-01,  2.1139747e-01,  3.1991774e-01, -1.5930286e-01,\n",
       "          4.5416230e-01,  1.9101876e-01],\n",
       "        [-1.3850933e-01, -4.7261602e-01, -8.1302345e-02, -1.9556823e-01,\n",
       "         -5.1578259e-01,  5.0523561e-01,  4.0316707e-01,  1.3616741e-01,\n",
       "          3.6193043e-01, -9.3032122e-03]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The variables are also accessible through nice accessors\n",
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建自己的layer(主要利用继承）\n",
    "在layer层下来建立自己的层\n",
    "\n",
    "我们可以通过改进keras的layer来实现自定义个功能。\\\n",
    "主要通过改进3个部分，来实现改进layer。\n",
    "1. init: 在这个部分中 你可以初始化你的神经网络，但这个初始化通常都是定值，不可改变。\n",
    "2. build：在这里，你可以通过传参，来实现输入输出的维度匹配，增加flexiblity\n",
    "3. call: 设计foward computation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(9, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer_7/kernel:0' shape=(3, 10) dtype=float32, numpy=\n",
      "array([[-0.02890015,  0.19478112, -0.48672318,  0.28419846, -0.20622617,\n",
      "         0.1434285 , -0.0997535 , -0.26356834,  0.10440004,  0.63498354],\n",
      "       [-0.2394411 , -0.2055232 ,  0.5110239 ,  0.02463537,  0.654497  ,\n",
      "         0.06378531,  0.06427515,  0.25407964, -0.6349515 ,  0.16968268],\n",
      "       [-0.09258163,  0.347324  , -0.09209311,  0.23933887, -0.5352801 ,\n",
      "        -0.56300604,  0.34887993,  0.00735098,  0.03096676,  0.27914405]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    #init通常用来定义一些不需要改变的初始化，例如output的维度，\n",
    "    #当然，你也可以定义input的维度，只是定义过后input的维度就不能随着输入维度的变化而变化\n",
    "    #因此这个类将不在有过多的flexiable\n",
    "    \n",
    "    \n",
    "    ##init函数会在实例化的时候被执行\n",
    "    \n",
    "  def __init__(self, num_outputs):         #自定义init，即需要传出的output的维度\n",
    "    \n",
    "    super(MyDenseLayer, self).__init__() #要继承父亲全部的constructor！\n",
    "    self.num_outputs = num_outputs       #自己再来定义一个必须传参。\n",
    "    \n",
    "    #build是用来初始化 init没定义完的部分\n",
    "    #而这部分最大的优点在于，build是在input到达的时候才被调用，而不是创建之初被确定下来的。\n",
    "    #这就让input的维度可以随着input data的变换而变换。\n",
    " \n",
    "\n",
    "\n",
    "    ##build和call函数均是在call obj的时候才会被执行。\n",
    "    #其中，build负责先建立kernel的维度，call负责计算前向计算。\n",
    "   \n",
    "  def build(self, input_shape): #input shape\n",
    "    \n",
    "    #利用父类的函数self.add_variable来增加一个层数！！\n",
    "    self.kernel = self.add_variable(\"kernel\",\n",
    "                                    shape=[int(input_shape[-1]),#input的 **最后一个维度**是kernel的第一维度\n",
    "                                           self.num_outputs])#output是kernel的第二维度\n",
    "                        #例如input（a,b) output维度是c\n",
    "                        #kernel的维度是 （input最后一个维度，output维度）= （b,c)\n",
    "                        #因此，矩阵的计算为， (a,b)*(b,c)=(a,c)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    #call是用来定义kernel的。即forward 的计算部分，。\n",
    "    \n",
    "  def call(self, input):\n",
    "    return tf.matmul(input, self.kernel)\n",
    "\n",
    "        # 计算是 input与kernel相乘\n",
    "        # kernel的维度是\n",
    "\n",
    "layer = MyDenseLayer(10)\n",
    "print(layer(tf.zeros([9, 3])))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建自己的Model\n",
    "我们可以在layer层面上，建立自己的keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block_6/conv2d_18/kernel:0', 'resnet_identity_block_6/conv2d_18/bias:0', 'resnet_identity_block_6/batch_normalization_18/gamma:0', 'resnet_identity_block_6/batch_normalization_18/beta:0', 'resnet_identity_block_6/conv2d_19/kernel:0', 'resnet_identity_block_6/conv2d_19/bias:0', 'resnet_identity_block_6/batch_normalization_19/gamma:0', 'resnet_identity_block_6/batch_normalization_19/beta:0', 'resnet_identity_block_6/conv2d_20/kernel:0', 'resnet_identity_block_6/conv2d_20/bias:0', 'resnet_identity_block_6/batch_normalization_20/gamma:0', 'resnet_identity_block_6/batch_normalization_20/beta:0']\n"
     ]
    }
   ],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    \n",
    "    #我们先在__init__里面初始化各种layer，在实例化的时候执行\n",
    "    \n",
    "  def __init__(self, kernel_size, filters): #filter传入的是list 表示各个filter的个数\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters  #将filter的个数都分一下\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    \n",
    "    #而后在call制作数据传递的流程\n",
    "    #x通过不断的自我更新来达到被处理的效果\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    #x就是个中间变量，x是上一个的输出，下一个输入，反复的覆盖，最后返回x，\n",
    "    \n",
    "    x = self.conv2a(input_tensor)#input进来，通过卷积后赋值给x\n",
    "    x = self.bn2a(x, training=training)#x进入bn2继续更新\n",
    "    x = tf.nn.relu(x)#再到relu\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x += input_tensor#x+input_tensor = x\n",
    "    return tf.nn.relu(x)#返回relu处理过后的x\n",
    "\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3]) \n",
    "                    #实例化model，kernel_size=1, filter1的个数是1， filter2=2，filter3=3\n",
    "                    #实例化的过程，只执行了class中的__init__部分\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "                    #这时候执行了call部分\n",
    "                    #input_tensor = tf.zeros([1,2,3,3])\n",
    "print([x.name for x in block.trainable_variables])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用这个和上面的功能是完全一样的\n",
    "my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1),\n",
    "                                                    input_shape=(\n",
    "                                                        None, None, 3)),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Conv2D(2, 1,\n",
    "                                                    padding='same'),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Conv2D(3, (1, 1)),\n",
    "                             tf.keras.layers.BatchNormalization()])\n",
    "my_seq(tf.zeros([1, 2, 3, 3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
