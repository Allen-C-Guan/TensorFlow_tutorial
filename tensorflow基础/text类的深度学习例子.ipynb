{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset from keras\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索data的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n",
      "train data:[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
      "train labels:0\n"
     ]
    }
   ],
   "source": [
    "#探索data\n",
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
    "print(\"train data:{}\".format(train_data[1]))\n",
    "print(\"train labels:{}\".format(train_labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以发现，所有字符都已经转换成对应都int形式了。  \n",
    "而labels用 0 1表示积极或者消极。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of data is:<class 'numpy.ndarray'>:\n",
      "the type of train_data in every cell in numpy:<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"the type of data is:{}:\".format(type(train_data)))\n",
    "print(\"the type of train_data in every cell in numpy:{}\".format(\n",
    "    type(train_data[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data的类型是numpy，而numpy中每个cell中放的是一个list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of first list in numpy cell:189,218,141\n"
     ]
    }
   ],
   "source": [
    "print(\"the length of first list in numpy cell:{},{},{}\".format(\n",
    "    len(train_data[1]),len(train_data[0]),len(train_data[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个list的长度还不尽相同，这是由于影评的长度不同导致的的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网上有现成的word和integer dict格式的mapping\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "# dict的赋值，dict[key]=value\n",
    "\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    #dict.items()是一对一对的拿出来\n",
    "    #for(a,b)  in (D,C)会把D,C一对一对拿出来，赋值给a和b，\n",
    "    #dict([(a,b),(c,d)])是分别把两对放到dict之中。\n",
    "    #因此 dict([(value,key)for (key,value) in word_index.items()])表示\n",
    "        #把word_index一对一对拿出来，而后赋值给变量（key，value），\n",
    "        #而后（key，value）对应赋值给（value，key）完成key和value的对换\n",
    "        #对换之后的pair，放在reverse_word_index之中。\n",
    "        #我们得到的就是原来dict的reverse\n",
    "\n",
    "        \n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i) for i in text])\n",
    "    #dict.get（key）返回的是key对应的value，\n",
    "    #'str'.join(sequence)把sequence中的每个元素，都以str为链接（间隔链接起来）\n",
    "    #' '.join()表示以空格相连。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到这是一个dict格式的，左边是words，右边是对应的int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " dict1 = {'firstname':'ma', 'lastname':'yun'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用pad_sequences来将所有数据均padding到同一个长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对test和train的input都padding到同一个长度。\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],padding='post',maxlen=256)\n",
    "        # value是padding的内容\n",
    "        # padding是padding在前面还是padding在后面\n",
    "        # maxlen表示最大的长度\n",
    "        \n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,value=word_index[\"<PAD>\"],padding='post',maxlen=256)\n",
    "\n",
    "\n",
    "#这里padding以后，每一行都是np的格式了，不是list了！！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，所有不够长度的sequence，在结尾都padding上了0，而0对应PAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
      "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
      "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
      "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
      "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
      "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
      "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
      "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
      " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
      "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
      "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
      " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
      "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
      "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16,\n",
    "                             kernel_regularizer=keras.regularizers.l2(0.01), \n",
    "                             activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "#我们需要加一个正则化防止过度拟合\n",
    "#l2(0.001) 表示层的权重矩阵中的每个系数都会将\n",
    "#0.001 * weight_coefficient_value**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层是Embedding层。该层会在整数编码的词汇表中查找每个字词-索引的嵌入向量。模型在接受训练时会学习这些向量。这些向量会向输出数组添加一个维度。生成的维度为：(batch, sequence, embedding)。\n",
    "\n",
    "接下来，一个GlobalAveragePooling1D层通过对序列维度求平均值，针对每个样本返回一个长度固定的输出向量。这样，模型便能够以尽可能简单的方式处理各种长度的输入。\n",
    "目的在于减少参数，防止过度拟合。\n",
    "\n",
    "该长度固定的输出向量会传入一个全连接( Dense)层（包含16个隐藏单元）。\n",
    "\n",
    "最后一层与单个输出节点密集连接。应用sigmoid激活函数后，结果是介于0到1之间的浮点值，表示概率或置信水平。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 21:51:42.363922 4743222720 deprecation.py:323] From /Users/allen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "        #在train_data中的第一维度中，从0到9999都拿过来\n",
    "partial_x_train = train_data[10000:]\n",
    "        #在train_data中第一维度中从10000到最后都拿过来\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型（fit）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 - 1s - loss: 0.8446 - acc: 0.6411 - val_loss: 0.8271 - val_acc: 0.7307\n",
      "Epoch 2/40\n",
      "15000/15000 - 1s - loss: 0.8124 - acc: 0.7195 - val_loss: 0.7978 - val_acc: 0.7416\n",
      "Epoch 3/40\n",
      "15000/15000 - 1s - loss: 0.7847 - acc: 0.7585 - val_loss: 0.7726 - val_acc: 0.7344\n",
      "Epoch 4/40\n",
      "15000/15000 - 1s - loss: 0.7609 - acc: 0.7501 - val_loss: 0.7508 - val_acc: 0.7304\n",
      "Epoch 5/40\n",
      "15000/15000 - 1s - loss: 0.7395 - acc: 0.7640 - val_loss: 0.7311 - val_acc: 0.7492\n",
      "Epoch 6/40\n",
      "15000/15000 - 1s - loss: 0.7195 - acc: 0.7671 - val_loss: 0.7124 - val_acc: 0.7631\n",
      "Epoch 7/40\n",
      "15000/15000 - 1s - loss: 0.6993 - acc: 0.7794 - val_loss: 0.6930 - val_acc: 0.7431\n",
      "Epoch 8/40\n",
      "15000/15000 - 1s - loss: 0.6779 - acc: 0.7817 - val_loss: 0.6727 - val_acc: 0.7754\n",
      "Epoch 9/40\n",
      "15000/15000 - 1s - loss: 0.6542 - acc: 0.7971 - val_loss: 0.6502 - val_acc: 0.7801\n",
      "Epoch 10/40\n",
      "15000/15000 - 1s - loss: 0.6288 - acc: 0.8054 - val_loss: 0.6269 - val_acc: 0.7896\n",
      "Epoch 11/40\n",
      "15000/15000 - 1s - loss: 0.6024 - acc: 0.8168 - val_loss: 0.6033 - val_acc: 0.8035\n",
      "Epoch 12/40\n",
      "15000/15000 - 1s - loss: 0.5759 - acc: 0.8279 - val_loss: 0.5812 - val_acc: 0.8153\n",
      "Epoch 13/40\n",
      "15000/15000 - 1s - loss: 0.5511 - acc: 0.8385 - val_loss: 0.5607 - val_acc: 0.8236\n",
      "Epoch 14/40\n",
      "15000/15000 - 1s - loss: 0.5274 - acc: 0.8510 - val_loss: 0.5400 - val_acc: 0.8328\n",
      "Epoch 15/40\n",
      "15000/15000 - 1s - loss: 0.5060 - acc: 0.8589 - val_loss: 0.5221 - val_acc: 0.8394\n",
      "Epoch 16/40\n",
      "15000/15000 - 1s - loss: 0.4868 - acc: 0.8677 - val_loss: 0.5067 - val_acc: 0.8450\n",
      "Epoch 17/40\n",
      "15000/15000 - 1s - loss: 0.4697 - acc: 0.8744 - val_loss: 0.4936 - val_acc: 0.8483\n",
      "Epoch 18/40\n",
      "15000/15000 - 1s - loss: 0.4548 - acc: 0.8777 - val_loss: 0.4811 - val_acc: 0.8545\n",
      "Epoch 19/40\n",
      "15000/15000 - 1s - loss: 0.4412 - acc: 0.8845 - val_loss: 0.4705 - val_acc: 0.8569\n",
      "Epoch 20/40\n",
      "15000/15000 - 1s - loss: 0.4288 - acc: 0.8869 - val_loss: 0.4616 - val_acc: 0.8591\n",
      "Epoch 21/40\n",
      "15000/15000 - 1s - loss: 0.4179 - acc: 0.8894 - val_loss: 0.4528 - val_acc: 0.8604\n",
      "Epoch 22/40\n",
      "15000/15000 - 1s - loss: 0.4078 - acc: 0.8929 - val_loss: 0.4452 - val_acc: 0.8626\n",
      "Epoch 23/40\n",
      "15000/15000 - 1s - loss: 0.3980 - acc: 0.8967 - val_loss: 0.4384 - val_acc: 0.8653\n",
      "Epoch 24/40\n",
      "15000/15000 - 1s - loss: 0.3895 - acc: 0.8987 - val_loss: 0.4314 - val_acc: 0.8683\n",
      "Epoch 25/40\n",
      "15000/15000 - 1s - loss: 0.3813 - acc: 0.9004 - val_loss: 0.4257 - val_acc: 0.8695\n",
      "Epoch 26/40\n",
      "15000/15000 - 1s - loss: 0.3737 - acc: 0.9025 - val_loss: 0.4202 - val_acc: 0.8695\n",
      "Epoch 27/40\n",
      "15000/15000 - 1s - loss: 0.3666 - acc: 0.9049 - val_loss: 0.4158 - val_acc: 0.8698\n",
      "Epoch 28/40\n",
      "15000/15000 - 1s - loss: 0.3598 - acc: 0.9065 - val_loss: 0.4104 - val_acc: 0.8736\n",
      "Epoch 29/40\n",
      "15000/15000 - 1s - loss: 0.3536 - acc: 0.9080 - val_loss: 0.4062 - val_acc: 0.8744\n",
      "Epoch 30/40\n",
      "15000/15000 - 1s - loss: 0.3477 - acc: 0.9089 - val_loss: 0.4024 - val_acc: 0.8748\n",
      "Epoch 31/40\n",
      "15000/15000 - 1s - loss: 0.3415 - acc: 0.9122 - val_loss: 0.3983 - val_acc: 0.8763\n",
      "Epoch 32/40\n",
      "15000/15000 - 1s - loss: 0.3361 - acc: 0.9134 - val_loss: 0.3949 - val_acc: 0.8774\n",
      "Epoch 33/40\n",
      "15000/15000 - 1s - loss: 0.3308 - acc: 0.9157 - val_loss: 0.3915 - val_acc: 0.8771\n",
      "Epoch 34/40\n",
      "15000/15000 - 1s - loss: 0.3257 - acc: 0.9175 - val_loss: 0.3882 - val_acc: 0.8796\n",
      "Epoch 35/40\n",
      "15000/15000 - 1s - loss: 0.3210 - acc: 0.9183 - val_loss: 0.3851 - val_acc: 0.8801\n",
      "Epoch 36/40\n",
      "15000/15000 - 1s - loss: 0.3161 - acc: 0.9205 - val_loss: 0.3824 - val_acc: 0.8806\n",
      "Epoch 37/40\n",
      "15000/15000 - 1s - loss: 0.3116 - acc: 0.9214 - val_loss: 0.3799 - val_acc: 0.8804\n",
      "Epoch 38/40\n",
      "15000/15000 - 1s - loss: 0.3072 - acc: 0.9227 - val_loss: 0.3771 - val_acc: 0.8814\n",
      "Epoch 39/40\n",
      "15000/15000 - 1s - loss: 0.3034 - acc: 0.9240 - val_loss: 0.3751 - val_acc: 0.8791\n",
      "Epoch 40/40\n",
      "15000/15000 - 1s - loss: 0.2992 - acc: 0.9251 - val_loss: 0.3728 - val_acc: 0.8817\n"
     ]
    }
   ],
   "source": [
    "#fitting\n",
    "history =  model.fit(partial_x_train,\n",
    "                     partial_y_train,\n",
    "                     epochs=40,\n",
    "                     batch_size=521,\n",
    "                     validation_data=(x_val,y_val),\n",
    "                     verbose=2)\n",
    "#verbose=1表示有进度条，=2表示没有进度条\n",
    "\n",
    "#在这里我们把事先对训练集分好的累，一部分用来训练，一部分用来validation（验证）\n",
    "#validation的作用：是阶段性的用验证集去验证目前的模型，然后时时检测test的准确率（不是学习的准确率）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 32us/sample - loss: 0.3860 - acc: 0.8727\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制history training表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8445629472017289,\n",
       "  0.8123972925742468,\n",
       "  0.7846877524733543,\n",
       "  0.7608531836867333,\n",
       "  0.73950779389143,\n",
       "  0.7194977412184079,\n",
       "  0.6993470727443695,\n",
       "  0.6778949726700783,\n",
       "  0.6542453737298648,\n",
       "  0.628831838798523,\n",
       "  0.6024004062533379,\n",
       "  0.5758567977190018,\n",
       "  0.5511010172526042,\n",
       "  0.5274073405146599,\n",
       "  0.5060298753559589,\n",
       "  0.48680996241172153,\n",
       "  0.46969967252810796,\n",
       "  0.4547730194071929,\n",
       "  0.44121991290251417,\n",
       "  0.4287877304474513,\n",
       "  0.41789221448898317,\n",
       "  0.4077726900378863,\n",
       "  0.3980354038317998,\n",
       "  0.38951385747591655,\n",
       "  0.3812945813894272,\n",
       "  0.3737330314397812,\n",
       "  0.3666029779752096,\n",
       "  0.3597504674931367,\n",
       "  0.35356907620628675,\n",
       "  0.34770783880154293,\n",
       "  0.34152967579960825,\n",
       "  0.3360721728463968,\n",
       "  0.3307841690202554,\n",
       "  0.32571353587110835,\n",
       "  0.32103746181527776,\n",
       "  0.31612262216210363,\n",
       "  0.3116389396707217,\n",
       "  0.30721321762402853,\n",
       "  0.30340988956888515,\n",
       "  0.2991524255156517],\n",
       " 'acc': [0.6411333,\n",
       "  0.7195333,\n",
       "  0.75846666,\n",
       "  0.75006664,\n",
       "  0.764,\n",
       "  0.76713336,\n",
       "  0.7794,\n",
       "  0.7816667,\n",
       "  0.7970667,\n",
       "  0.8054,\n",
       "  0.8168,\n",
       "  0.8278667,\n",
       "  0.83853334,\n",
       "  0.851,\n",
       "  0.8589333,\n",
       "  0.86773336,\n",
       "  0.8744,\n",
       "  0.87766665,\n",
       "  0.88446665,\n",
       "  0.8868667,\n",
       "  0.8894,\n",
       "  0.8928667,\n",
       "  0.89666665,\n",
       "  0.8987333,\n",
       "  0.9004,\n",
       "  0.90253335,\n",
       "  0.9048667,\n",
       "  0.90646666,\n",
       "  0.908,\n",
       "  0.90893334,\n",
       "  0.9122,\n",
       "  0.9134,\n",
       "  0.91566664,\n",
       "  0.91753334,\n",
       "  0.91833335,\n",
       "  0.92053336,\n",
       "  0.9214,\n",
       "  0.92266667,\n",
       "  0.924,\n",
       "  0.92506665],\n",
       " 'val_loss': [0.827143019914627,\n",
       "  0.7977817440211773,\n",
       "  0.772602083337307,\n",
       "  0.7507921885848046,\n",
       "  0.7311443336725235,\n",
       "  0.7124145916342736,\n",
       "  0.6930397858917713,\n",
       "  0.6726760084092617,\n",
       "  0.6501993425488471,\n",
       "  0.626874946820736,\n",
       "  0.6033237253129482,\n",
       "  0.5812467600226402,\n",
       "  0.5607036750078201,\n",
       "  0.5400271004736423,\n",
       "  0.5221094522953034,\n",
       "  0.5067275317847729,\n",
       "  0.4936162671267986,\n",
       "  0.48112358348965645,\n",
       "  0.47050156362354756,\n",
       "  0.46162101939022543,\n",
       "  0.4527966465473175,\n",
       "  0.44521436869204045,\n",
       "  0.4384455619663,\n",
       "  0.4314480949729681,\n",
       "  0.425681631308794,\n",
       "  0.42023785002827646,\n",
       "  0.41576061014533044,\n",
       "  0.41040327809154986,\n",
       "  0.4062402866452932,\n",
       "  0.40241767829358577,\n",
       "  0.398327864921093,\n",
       "  0.3949055246859789,\n",
       "  0.3915093660980463,\n",
       "  0.38821610325872896,\n",
       "  0.3850953205317259,\n",
       "  0.3823706698834896,\n",
       "  0.37990897720754147,\n",
       "  0.37710011602938176,\n",
       "  0.37508424049913885,\n",
       "  0.3727608641862869],\n",
       " 'val_acc': [0.7307,\n",
       "  0.7416,\n",
       "  0.7344,\n",
       "  0.7304,\n",
       "  0.7492,\n",
       "  0.7631,\n",
       "  0.7431,\n",
       "  0.7754,\n",
       "  0.7801,\n",
       "  0.7896,\n",
       "  0.8035,\n",
       "  0.8153,\n",
       "  0.8236,\n",
       "  0.8328,\n",
       "  0.8394,\n",
       "  0.845,\n",
       "  0.8483,\n",
       "  0.8545,\n",
       "  0.8569,\n",
       "  0.8591,\n",
       "  0.8604,\n",
       "  0.8626,\n",
       "  0.8653,\n",
       "  0.8683,\n",
       "  0.8695,\n",
       "  0.8695,\n",
       "  0.8698,\n",
       "  0.8736,\n",
       "  0.8744,\n",
       "  0.8748,\n",
       "  0.8763,\n",
       "  0.8774,\n",
       "  0.8771,\n",
       "  0.8796,\n",
       "  0.8801,\n",
       "  0.8806,\n",
       "  0.8804,\n",
       "  0.8814,\n",
       "  0.8791,\n",
       "  0.8817]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n",
    "#这是一个dict类型的文件，记录了所有training过程中对于准确度的记录per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict =  history.history\n",
    "history_dict.keys()#dict.key()可以返回所有的keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(history_dict['loss'])\n",
    "##dict的value是list形的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在该层，随机的丢弃几个输出的特性，ie，让一个向量的第一和第三个项的值 = 0，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "\n",
    "model_drop = keras.Sequential()\n",
    "model_drop.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model_drop.add(keras.layers.GlobalAveragePooling1D())\n",
    "model_drop.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model_drop.add(keras.layers.Dropout(0.4))\n",
    "model_drop.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "15000/15000 - 1s - loss: 0.1166 - acc: 0.9686 - val_loss: 0.3049 - val_acc: 0.8846\n",
      "Epoch 2/100\n",
      "15000/15000 - 1s - loss: 0.1123 - acc: 0.9692 - val_loss: 0.3074 - val_acc: 0.8847\n",
      "Epoch 3/100\n",
      "15000/15000 - 1s - loss: 0.1098 - acc: 0.9724 - val_loss: 0.3111 - val_acc: 0.8827\n",
      "Epoch 4/100\n",
      "15000/15000 - 1s - loss: 0.1059 - acc: 0.9726 - val_loss: 0.3139 - val_acc: 0.8826\n",
      "Epoch 5/100\n",
      "15000/15000 - 1s - loss: 0.1022 - acc: 0.9747 - val_loss: 0.3178 - val_acc: 0.8815\n",
      "Epoch 6/100\n",
      "15000/15000 - 1s - loss: 0.0984 - acc: 0.9747 - val_loss: 0.3209 - val_acc: 0.8813\n",
      "Epoch 7/100\n",
      "15000/15000 - 1s - loss: 0.0950 - acc: 0.9765 - val_loss: 0.3238 - val_acc: 0.8817\n",
      "Epoch 8/100\n",
      "15000/15000 - 1s - loss: 0.0928 - acc: 0.9771 - val_loss: 0.3277 - val_acc: 0.8810\n",
      "Epoch 9/100\n",
      "15000/15000 - 1s - loss: 0.0884 - acc: 0.9783 - val_loss: 0.3333 - val_acc: 0.8795\n",
      "Epoch 10/100\n",
      "15000/15000 - 1s - loss: 0.0866 - acc: 0.9795 - val_loss: 0.3359 - val_acc: 0.8798\n",
      "Epoch 11/100\n",
      "15000/15000 - 1s - loss: 0.0830 - acc: 0.9794 - val_loss: 0.3395 - val_acc: 0.8795\n",
      "Epoch 12/100\n",
      "15000/15000 - 1s - loss: 0.0798 - acc: 0.9812 - val_loss: 0.3455 - val_acc: 0.8793\n",
      "Epoch 13/100\n",
      "15000/15000 - 1s - loss: 0.0796 - acc: 0.9816 - val_loss: 0.3476 - val_acc: 0.8786\n",
      "Epoch 14/100\n",
      "15000/15000 - 1s - loss: 0.0750 - acc: 0.9833 - val_loss: 0.3519 - val_acc: 0.8788\n",
      "Epoch 15/100\n",
      "15000/15000 - 1s - loss: 0.0725 - acc: 0.9840 - val_loss: 0.3571 - val_acc: 0.8774\n",
      "Epoch 16/100\n",
      "15000/15000 - 1s - loss: 0.0715 - acc: 0.9840 - val_loss: 0.3593 - val_acc: 0.8780\n",
      "Epoch 17/100\n",
      "15000/15000 - 1s - loss: 0.0700 - acc: 0.9860 - val_loss: 0.3640 - val_acc: 0.8769\n",
      "Epoch 18/100\n",
      "15000/15000 - 1s - loss: 0.0668 - acc: 0.9866 - val_loss: 0.3685 - val_acc: 0.8764\n",
      "Epoch 19/100\n",
      "15000/15000 - 1s - loss: 0.0654 - acc: 0.9867 - val_loss: 0.3737 - val_acc: 0.8775\n",
      "Epoch 20/100\n",
      "15000/15000 - 1s - loss: 0.0611 - acc: 0.9887 - val_loss: 0.3783 - val_acc: 0.8762\n",
      "Epoch 21/100\n",
      "15000/15000 - 1s - loss: 0.0626 - acc: 0.9871 - val_loss: 0.3826 - val_acc: 0.8755\n",
      "Epoch 22/100\n",
      "15000/15000 - 1s - loss: 0.0598 - acc: 0.9883 - val_loss: 0.3873 - val_acc: 0.8758\n",
      "Epoch 23/100\n",
      "15000/15000 - 1s - loss: 0.0560 - acc: 0.9894 - val_loss: 0.3932 - val_acc: 0.8753\n",
      "Epoch 24/100\n",
      "15000/15000 - 1s - loss: 0.0551 - acc: 0.9896 - val_loss: 0.3978 - val_acc: 0.8747\n",
      "Epoch 25/100\n",
      "15000/15000 - 1s - loss: 0.0528 - acc: 0.9898 - val_loss: 0.4042 - val_acc: 0.8743\n",
      "Epoch 26/100\n",
      "15000/15000 - 1s - loss: 0.0524 - acc: 0.9897 - val_loss: 0.4088 - val_acc: 0.8748\n",
      "Epoch 27/100\n",
      "15000/15000 - 1s - loss: 0.0512 - acc: 0.9913 - val_loss: 0.4130 - val_acc: 0.8738\n",
      "Epoch 28/100\n",
      "15000/15000 - 1s - loss: 0.0463 - acc: 0.9917 - val_loss: 0.4194 - val_acc: 0.8744\n",
      "Epoch 29/100\n",
      "15000/15000 - 1s - loss: 0.0485 - acc: 0.9913 - val_loss: 0.4263 - val_acc: 0.8726\n",
      "Epoch 30/100\n",
      "15000/15000 - 1s - loss: 0.0458 - acc: 0.9929 - val_loss: 0.4293 - val_acc: 0.8734\n",
      "Epoch 31/100\n",
      "15000/15000 - 1s - loss: 0.0441 - acc: 0.9919 - val_loss: 0.4342 - val_acc: 0.8736\n",
      "Epoch 32/100\n",
      "15000/15000 - 1s - loss: 0.0433 - acc: 0.9933 - val_loss: 0.4407 - val_acc: 0.8718\n",
      "Epoch 33/100\n",
      "15000/15000 - 1s - loss: 0.0415 - acc: 0.9939 - val_loss: 0.4462 - val_acc: 0.8726\n",
      "Epoch 34/100\n",
      "15000/15000 - 1s - loss: 0.0401 - acc: 0.9933 - val_loss: 0.4513 - val_acc: 0.8723\n",
      "Epoch 35/100\n",
      "15000/15000 - 1s - loss: 0.0389 - acc: 0.9940 - val_loss: 0.4556 - val_acc: 0.8724\n",
      "Epoch 36/100\n",
      "15000/15000 - 1s - loss: 0.0368 - acc: 0.9954 - val_loss: 0.4618 - val_acc: 0.8723\n",
      "Epoch 37/100\n",
      "15000/15000 - 1s - loss: 0.0368 - acc: 0.9945 - val_loss: 0.4655 - val_acc: 0.8702\n",
      "Epoch 38/100\n",
      "15000/15000 - 1s - loss: 0.0360 - acc: 0.9949 - val_loss: 0.4717 - val_acc: 0.8703\n",
      "Epoch 39/100\n",
      "15000/15000 - 1s - loss: 0.0341 - acc: 0.9957 - val_loss: 0.4798 - val_acc: 0.8708\n",
      "Epoch 40/100\n",
      "15000/15000 - 1s - loss: 0.0324 - acc: 0.9952 - val_loss: 0.4852 - val_acc: 0.8704\n",
      "Epoch 41/100\n",
      "15000/15000 - 1s - loss: 0.0300 - acc: 0.9961 - val_loss: 0.4929 - val_acc: 0.8697\n",
      "Epoch 42/100\n",
      "15000/15000 - 1s - loss: 0.0308 - acc: 0.9957 - val_loss: 0.4959 - val_acc: 0.8687\n",
      "Epoch 43/100\n",
      "15000/15000 - 1s - loss: 0.0307 - acc: 0.9952 - val_loss: 0.5028 - val_acc: 0.8684\n",
      "Epoch 44/100\n",
      "15000/15000 - 1s - loss: 0.0291 - acc: 0.9966 - val_loss: 0.5097 - val_acc: 0.8678\n",
      "Epoch 45/100\n",
      "15000/15000 - 1s - loss: 0.0288 - acc: 0.9959 - val_loss: 0.5132 - val_acc: 0.8679\n",
      "Epoch 46/100\n",
      "15000/15000 - 1s - loss: 0.0274 - acc: 0.9963 - val_loss: 0.5200 - val_acc: 0.8679\n",
      "Epoch 47/100\n",
      "15000/15000 - 1s - loss: 0.0276 - acc: 0.9964 - val_loss: 0.5264 - val_acc: 0.8676\n",
      "Epoch 48/100\n",
      "15000/15000 - 1s - loss: 0.0254 - acc: 0.9967 - val_loss: 0.5311 - val_acc: 0.8674\n",
      "Epoch 49/100\n",
      "15000/15000 - 1s - loss: 0.0249 - acc: 0.9969 - val_loss: 0.5359 - val_acc: 0.8668\n",
      "Epoch 50/100\n",
      "15000/15000 - 1s - loss: 0.0242 - acc: 0.9965 - val_loss: 0.5431 - val_acc: 0.8667\n",
      "Epoch 51/100\n",
      "15000/15000 - 1s - loss: 0.0235 - acc: 0.9973 - val_loss: 0.5480 - val_acc: 0.8660\n",
      "Epoch 52/100\n",
      "15000/15000 - 1s - loss: 0.0214 - acc: 0.9978 - val_loss: 0.5530 - val_acc: 0.8663\n",
      "Epoch 53/100\n",
      "15000/15000 - 1s - loss: 0.0222 - acc: 0.9973 - val_loss: 0.5581 - val_acc: 0.8657\n",
      "Epoch 54/100\n",
      "15000/15000 - 1s - loss: 0.0216 - acc: 0.9968 - val_loss: 0.5640 - val_acc: 0.8652\n",
      "Epoch 55/100\n",
      "15000/15000 - 1s - loss: 0.0201 - acc: 0.9976 - val_loss: 0.5696 - val_acc: 0.8657\n",
      "Epoch 56/100\n",
      "15000/15000 - 1s - loss: 0.0194 - acc: 0.9980 - val_loss: 0.5760 - val_acc: 0.8662\n",
      "Epoch 57/100\n",
      "15000/15000 - 1s - loss: 0.0193 - acc: 0.9980 - val_loss: 0.5812 - val_acc: 0.8656\n",
      "Epoch 58/100\n",
      "15000/15000 - 1s - loss: 0.0190 - acc: 0.9977 - val_loss: 0.5855 - val_acc: 0.8658\n",
      "Epoch 59/100\n",
      "15000/15000 - 1s - loss: 0.0183 - acc: 0.9980 - val_loss: 0.5913 - val_acc: 0.8659\n",
      "Epoch 60/100\n",
      "15000/15000 - 1s - loss: 0.0177 - acc: 0.9980 - val_loss: 0.5990 - val_acc: 0.8657\n",
      "Epoch 61/100\n",
      "15000/15000 - 1s - loss: 0.0169 - acc: 0.9983 - val_loss: 0.6042 - val_acc: 0.8660\n",
      "Epoch 62/100\n",
      "15000/15000 - 1s - loss: 0.0169 - acc: 0.9983 - val_loss: 0.6116 - val_acc: 0.8652\n",
      "Epoch 63/100\n",
      "15000/15000 - 1s - loss: 0.0163 - acc: 0.9985 - val_loss: 0.6153 - val_acc: 0.8652\n",
      "Epoch 64/100\n",
      "15000/15000 - 1s - loss: 0.0160 - acc: 0.9989 - val_loss: 0.6228 - val_acc: 0.8657\n",
      "Epoch 65/100\n",
      "15000/15000 - 1s - loss: 0.0157 - acc: 0.9984 - val_loss: 0.6275 - val_acc: 0.8651\n",
      "Epoch 66/100\n",
      "15000/15000 - 1s - loss: 0.0157 - acc: 0.9985 - val_loss: 0.6317 - val_acc: 0.8656\n",
      "Epoch 67/100\n",
      "15000/15000 - 1s - loss: 0.0149 - acc: 0.9984 - val_loss: 0.6367 - val_acc: 0.8657\n",
      "Epoch 68/100\n",
      "15000/15000 - 1s - loss: 0.0141 - acc: 0.9987 - val_loss: 0.6435 - val_acc: 0.8652\n",
      "Epoch 69/100\n",
      "15000/15000 - 1s - loss: 0.0134 - acc: 0.9990 - val_loss: 0.6499 - val_acc: 0.8650\n",
      "Epoch 70/100\n",
      "15000/15000 - 1s - loss: 0.0138 - acc: 0.9987 - val_loss: 0.6574 - val_acc: 0.8649\n",
      "Epoch 71/100\n",
      "15000/15000 - 1s - loss: 0.0130 - acc: 0.9991 - val_loss: 0.6627 - val_acc: 0.8648\n",
      "Epoch 72/100\n",
      "15000/15000 - 1s - loss: 0.0125 - acc: 0.9989 - val_loss: 0.6673 - val_acc: 0.8641\n",
      "Epoch 73/100\n",
      "15000/15000 - 1s - loss: 0.0120 - acc: 0.9985 - val_loss: 0.6730 - val_acc: 0.8648\n",
      "Epoch 74/100\n",
      "15000/15000 - 1s - loss: 0.0127 - acc: 0.9989 - val_loss: 0.6776 - val_acc: 0.8647\n",
      "Epoch 75/100\n",
      "15000/15000 - 1s - loss: 0.0117 - acc: 0.9988 - val_loss: 0.6867 - val_acc: 0.8634\n",
      "Epoch 76/100\n",
      "15000/15000 - 1s - loss: 0.0108 - acc: 0.9993 - val_loss: 0.6894 - val_acc: 0.8647\n",
      "Epoch 77/100\n",
      "15000/15000 - 1s - loss: 0.0108 - acc: 0.9993 - val_loss: 0.6985 - val_acc: 0.8635\n",
      "Epoch 78/100\n",
      "15000/15000 - 1s - loss: 0.0110 - acc: 0.9987 - val_loss: 0.7055 - val_acc: 0.8631\n",
      "Epoch 79/100\n",
      "15000/15000 - 1s - loss: 0.0102 - acc: 0.9993 - val_loss: 0.7073 - val_acc: 0.8640\n",
      "Epoch 80/100\n",
      "15000/15000 - 1s - loss: 0.0099 - acc: 0.9993 - val_loss: 0.7144 - val_acc: 0.8634\n",
      "Epoch 81/100\n",
      "15000/15000 - 1s - loss: 0.0099 - acc: 0.9993 - val_loss: 0.7201 - val_acc: 0.8639\n",
      "Epoch 82/100\n",
      "15000/15000 - 1s - loss: 0.0100 - acc: 0.9986 - val_loss: 0.7242 - val_acc: 0.8633\n",
      "Epoch 83/100\n",
      "15000/15000 - 1s - loss: 0.0096 - acc: 0.9991 - val_loss: 0.7299 - val_acc: 0.8629\n",
      "Epoch 84/100\n",
      "15000/15000 - 1s - loss: 0.0096 - acc: 0.9990 - val_loss: 0.7316 - val_acc: 0.8628\n",
      "Epoch 85/100\n",
      "15000/15000 - 1s - loss: 0.0089 - acc: 0.9994 - val_loss: 0.7433 - val_acc: 0.8621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "15000/15000 - 1s - loss: 0.0088 - acc: 0.9992 - val_loss: 0.7497 - val_acc: 0.8627\n",
      "Epoch 87/100\n",
      "15000/15000 - 1s - loss: 0.0082 - acc: 0.9997 - val_loss: 0.7519 - val_acc: 0.8617\n",
      "Epoch 88/100\n",
      "15000/15000 - 1s - loss: 0.0083 - acc: 0.9997 - val_loss: 0.7606 - val_acc: 0.8627\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-41949530fa02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m521\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                      verbose=2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history =  model_drop.fit(partial_x_train,\n",
    "                     partial_y_train,\n",
    "                     epochs=40,\n",
    "                     batch_size=521,\n",
    "                     validation_data=(x_val,y_val),\n",
    "                     verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop layer可以有效的防止过度拟合，不过也会随着训练次数过多而产生过拟合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
